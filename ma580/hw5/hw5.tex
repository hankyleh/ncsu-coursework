\documentclass{template}

\title{MA 580 Assignment 4}
\author{Kyle Hansen}
\date{31 October 2025}

\usepackage{cancel}

\newcommand{\maxmag}{\mathrm{\mathbf{maxmag}}}
\newcommand{\minmag}{\mathrm{\mathbf{minmag}}}

\begin{document}

\maketitle

\textbf{AI Use Statement: ChatGPT was used to help recall the name and properties of the quadratic form in problem 2}

\section{GMRES Residual Estimate}

An estimate for GMRES residual is given by

\begin{equation}
  \frac{\norm{r_k}}{\norm{r_0}} \leq \norm{p_k(A)}
\end{equation}

for every $p_k \in \mathcal{P}_k$ ($p_k(0) = 1$). Then, for diagonizable matrix $A = V\Lambda V^{-1}$, 

\begin{equation}
  \frac{\norm{r_k}}{\norm{r_0}} \leq \norm{V p_k(\Lambda) V^T} \leq \norm{V}\norm{p_k(\Lambda)}\norm{V^{-1}}
\end{equation}

and since $\norm{V}\norm{V^{-1}} = \kappa_2(V)$, 

\begin{equation}
  \frac{\norm{r_k}}{\norm{r_0}} \leq \kappa_2(V)\norm{p_k(\Lambda)} = \kappa_2(V)\norm{\begin{bmatrix}
    p_k(\lambda_1) &                & & \\
                   & p_k(\lambda_2) & & \\
                   &                & \ddots & \\
                   &                &        & p(\lambda_n)
  \end{bmatrix}}
\end{equation}

Then, since the norm of a diagonal matrix is its largest eigenvalue, 

\begin{equation}
  \frac{\norm{r_k}}{\norm{r_0}} \leq \kappa_2(V) \max_{z \in \sigma(A)}|p_k(z)|.
\end{equation}

Then an appropriate choice of polynomial $p_k$ would be one that minimizes the the value about the eigenvalues of $A$. Here $(\frac{10 - z}{10}\frac{20-z}{20})^{k/2}$ is chosen, since $p_k = 0$ at the center of the range of possible eivenvalues for this problem. For this choice of $p_k$, the maximum absolute value $|\max p_k(z)|$ for $z \subset (9, 11) \cup (19, 21) $ occurs at $z=9$ and is equal to $(11/200)^{k/2}$. Then, for the given properties of $A$, the residual at iteration $k$ can be bounded by

\begin{equation}
  \frac{\norm{r_k}}{\norm{r_0}} \leq 100 \cdot \left(\frac{11}{200}\right)^{\frac{k}{2}}.
\end{equation}

Using a relative residual of $10^{-6}$, the number of iterations required is

\begin{align}
  10^{-6} &= 100 \cdot \left(\frac{11}{200}\right)^{\frac{k}{2}}\\
  \frac{10^{-6}}{100} &= \left(\frac{11}{200}\right)^{\frac{k}{2}}\\
  \ln 10^{-8} &= \frac{k}{2}\ln \frac{11}{200}\\
  2\frac{\ln 10^{-8}}{\ln \frac{11}{200}} &= k = 12.702
\end{align}

And thus about 13 iterations are required for the desired level of convergence.


\section{CG Error Estimate}

The CG iterative error is

\begin{align}
  e_k &= x^\star - x_k\\
    &= x^\star - x_0 - \sum_{j=0}^{k-1}r_j A^j r_0\\
    &= e_0 - \sum_{j=0}^{k-1}r_j A^j r_0 \leq p(A)e_0, \qquad \forall p \in \mathcal{P}_k,
\end{align}

and its norm is bounded by

\begin{align}
    \norm{e_k}_A \leq \norm{p(A)e_0}_A&\\
    \norm{e_k}_A^2 \leq \norm{p(A)e_0}_A^2 &= (p(A) e_0)^T A p(A) e_0.
\end{align}

Since $A = U \Lambda U^T$, where $\Lambda$ is a diagonal matrix composed of the eigenvalues of $A$ and $U$ is an orthonormal matrix composed of the corresponding eigenvectors. Then $p(A) = U p(\Lambda) U^T$ and

\begin{align}
  \norm{e_k}_A^2 &\leq (p(A)e_0)^T U \Lambda U^T (p(A)e_0)\\
                 &\leq (U p(\Lambda) U^T e_0)^T U \Lambda U^T (U p(\Lambda) U^T e_0)\\
                 &\leq e_0^T U p(\Lambda) \cancelto{I}{U^T U} \Lambda \cancelto{I}{U^T U} p(\Lambda) U^T e_0\\
                 &\leq e_0^T U p(\Lambda)  \Lambda  p(\Lambda) U^T e_0
\end{align}

This product is of quadratic form and can be represented as a sum

\begin{align}
  \norm{e_k}_A^2 &\leq (U^T e_0)^T p(\Lambda)  \Lambda  p(\Lambda) U^T e_0\\
                &\leq \sum_{i=0}^{n} \sum_{j=1}^{n} (U^T e_0)_i (p(\Lambda)  \Lambda  p(\Lambda))_{i,j}(U^T e_0)_j
\end{align}

where, since $p(\Lambda)  \Lambda  p(\Lambda)$ is a diagonal matrix,

\begin{align}
  \norm{e_k}_A^2 &\leq \sum_{j=1}^{n} (U^T e_0)_j \lambda_j p(\lambda_j)^2(U^T e_0)_j\\
  \norm{e_k}_A^2 &\leq \sum_{j=1}^{n} \lambda_j p(\lambda_j)^2  \ip{u_j}{e_0},
\end{align}

where $u_j$ are the columns of $U$, equal to the eigenvectors of $A$.


\section{Convergence Proof}

Here $x^\star$ denotes the exact solution $x^\star = A^{-1}b$. 

The CG iterative error can be estimated by

\begin{equation}
  \frac{\norm{e_k}_A}{\norm{e_0}_A} \leq \norm{p_k(A)}_A.
\end{equation}

Then one valid choice for the residual polynomial is

\begin{equation}
  \overline{p}_k(z) = \prod_{i=1}^{k} \frac{\lambda_i - z}{\lambda_i}
\end{equation}

where $p_k(0) = 1$ and $p_k(\lambda_i) = 0$ for $1\leq i\leq k$. Then, with matrix $V$ composed of the eigenvectors of $A$ and diagonal matrix $\Lambda$ where the diagonal entries are the corresponsing eigenvalues of $A$, $A = V\Lambda V^{-1}$, and $p_k(A) = Vp_K(\Lambda)V^{-1}$:

\begin{align}
  p_k(A) &= \sum_{j=0}^{k}c_j A^j\\
  p_k(A) &= \sum_{j=0}^{k}c_j V D^j V^{-1}\\
  p_k(A) &= V\left(\sum_{j=0}^{k}c_j D^j \right)V^{-1}\\
  p_k(A) = V p_K(\Lambda)V^{-1},
\end{align}

and since $\overline{p}_k(\lambda_i) = 0$ for $1 \leq i \leq k$, 

\begin{equation}
  \overline{p}_n(\lambda_l) = \frac{\lambda_1 - \lambda_l}{\lambda_1} \cdots \frac{\cancelto{0}{\lambda_l - \lambda_l}}{\lambda_l} \cdots \frac{\lambda_n - \lambda_l}{\lambda_n} = 0 \text{  for } 1\leq l \leq n
\end{equation}

and

\begin{equation}
  \frac{\norm{e_k}_A}{\norm{e_0}_A} \leq\norm{\overline{p}_n(A)}_A = \norm{V \overline{p}_n(\Lambda)V^-1}_A = 0
\end{equation}

Then the error of the $n$th iterate is zero, so the algorithm converges in at most $n$ iterations.
\section{CG Inverse Problem}

% TODO

\end{document}
